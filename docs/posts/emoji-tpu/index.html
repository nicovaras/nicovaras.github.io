<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">Emoji Journey: Exploring Sentiments with the Power of TPUs üéâ | Sir Gradient&#39;s Descent into Madness</title>
<meta property="og:title" content="Emoji Journey: Exploring Sentiments with the Power of TPUs üéâ | Sir Gradient&#39;s Descent into Madness" />
<meta name="twitter:title" content="Emoji Journey: Exploring Sentiments with the Power of TPUs üéâ | Sir Gradient&#39;s Descent into Madness" />
<meta itemprop="name" content="Emoji Journey: Exploring Sentiments with the Power of TPUs üéâ | Sir Gradient&#39;s Descent into Madness" />
<meta name="application-name" content="Emoji Journey: Exploring Sentiments with the Power of TPUs üéâ | Sir Gradient&#39;s Descent into Madness" />
<meta property="og:site_name" content="Sir Gradient&#39;s Descent into Madness" />

<meta name="description" content="Exploring the intricacies of Machine Learning and Data Science, one algorithm at a time.">
<meta itemprop="description" content="Exploring the intricacies of Machine Learning and Data Science, one algorithm at a time." />
<meta property="og:description" content="Exploring the intricacies of Machine Learning and Data Science, one algorithm at a time." />
<meta name="twitter:description" content="Exploring the intricacies of Machine Learning and Data Science, one algorithm at a time." />

<meta property="og:locale" content="en-us" />
<meta name="language" content="en-us" />



  <meta itemprop="image" content="https://nicovaras.github.io/" />
  <meta property="og:image" content="https://nicovaras.github.io/" />
  <meta name="twitter:image" content="https://nicovaras.github.io/" />
  <meta name="twitter:image:src" content="https://nicovaras.github.io/" />




    
    
    
    

    <meta property="og:type" content="article" />
    <meta property="og:article:published_time" content=2023-04-28T11:43:05&#43;0200 />
    <meta property="article:published_time" content=2023-04-28T11:43:05&#43;0200 />

    

    

    <script defer type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "Article",
        "headline": "Emoji Journey: Exploring Sentiments with the Power of TPUs üéâ",
        "author": {
        "@type": "Person",
        "name": ""
        },
        "datePublished": "2023-04-28",
        "description": "",
        "wordCount":  1658 ,
        "mainEntityOfPage": "True",
        "dateModified": "2023-04-28",
        "image": {
        "@type": "imageObject",
        "url": ""
        },
        "publisher": {
        "@type": "Organization",
        "name": "Sir Gradient\u0027s Descent into Madness"
        }
    }
    </script>

  <meta name="generator" content="Hugo 0.111.3">

  

  <link rel="canonical" href="https://nicovaras.github.io/posts/emoji-tpu/"><link href="/sass/main.min.0eebb6db90b4ec9f4444ef402f08421ee056025ba860df3d749a9f299d472008.css" rel="stylesheet"><link href="/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

  

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">


  
  <link rel="icon" type="image/svg+xml" href="/images/favicon/favicon.svg">

  
    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3RPCMX2ZR6"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-3RPCMX2ZR6', { 'anonymize_ip': false });
}
</script>
</head><body data-theme = "dark" class="notransition">
<script src="https://nicovaras.github.io/js/themeLoader.min.4e9e1a253d543bbfec02e7f2460d9621e719fd739dc8a5256faa91cda6e12e03.js"></script><div class="navbar" role="navigation">
  <nav class="menu" aria-label="Main Navigation">
    <a href="/" class="logo">
      <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title>Home</title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
    </a>
    <input type="checkbox" id="menu-trigger" class="menu-trigger" />
    <label for="menu-trigger">
      <span class="menu-icon">
        <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
      </span>
    </label>

    <div class="trigger">
      <ul class="trigger-container">
        
        
          <li>
            <a 
            class="menu-link "
            href="/">
            Home
            </a>
            
          </li>
        
          <li>
            <a 
            class="menu-link active"
            href="/posts/">
            Posts
            </a>
            
          </li>
        
        <li class="menu-separator">
          <span>|</span>
        </li>
      </ul>
      <a id="mode" href="#">
        <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
        <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
      </a>
    </div>
  </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">Emoji Journey: Exploring Sentiments with the Power of TPUs üéâ</h1>
                
                <div class="post-meta">
                    <time datetime="2023-04-28T11:43:05&#43;02:00" itemprop="datePublished"> Apr 28, 2023 </time>
                </div>
            </header>
            <div class="page-content">
                <p>Notebook: <a href="https://github.com/nicovaras/deep-learning-projects/blob/main/emoji.ipynb">here</a></p>
<p>Today, I&rsquo;m excited to share a fun mini project I&rsquo;ve been working on üéâ. I came across a dataset of tweets with emojis on Kaggle and thought, &ldquo;Why not try to do something with this?&rdquo; ü§î. The project started out pretty standard, but as there was a ton of data and my architectures were quite large (I tried transformers again without success - yet again üòÖ), it quickly became challenging. I attempted to run it on my local GPU, Kaggle, and Google Colab GPUs, but the processing times were just too much (like one hour per epoch üò´). That&rsquo;s when I decided to venture into something that perhaps you, my dear reader, haven&rsquo;t tried before: TPUs üí°. And guess what? This made a HUGE difference! üöÄ So, without further ado, let&rsquo;s dive right in!</p>
<h2 id="dataset-overview">Dataset Overview</h2>
<p>Alright, let&rsquo;s take a closer look at the dataset I&rsquo;m working with in this project üïµÔ∏è‚Äç‚ôÄÔ∏è. The dataset, called <a href="https://www.kaggle.com/datasets/rexhaif/emojifydata-en">emojifydata-en</a>, consists of millions of tweets, each containing emojis üòÑ. The data is split into four files: train, test, dev, and one file that has everything combined. To get started, I needed to do a bit of preprocessing, as the raw data wasn&rsquo;t exactly user-friendly. Here are a few example tweets before preprocessing:</p>
<pre tabindex="0"><code>&lt;START&gt; O
CeeC O
is O
going O
to O
be O
another O
Tboss O
What O
is O
45 O
million O
Naira üòÇ
&lt;STOP&gt; O

&lt;START&gt; O
This O
gif O
kills O
me O
Death O
is O
literally O
gushing O
towards O
you O
and O
you O
really O
gon O
do O
a O
whole O
3point O
turn üò©
&lt;STOP&gt; O

&lt;START&gt; O
LOVE O
TEST O
Raw O
Real O
JaDine üíú
&lt;STOP&gt; O
</code></pre><p>Yes, it&rsquo;s Twitter, and people don&rsquo;t always write with perfect grammar ü§∑. This adds an extra layer of challenge when creating a language model.</p>
<p>The dataset contains a total of 49 different emoji classes üåà. Initially, I attempted to create a classifier for all of these classes, but it quickly became apparent that I would need much more data to achieve accurate results. So, I decided to narrow it down to just 5 emojis, ensuring that they represented a diverse range of sentiments. The emojis I chose are: ü§¶, ü§£, üôè, üò© and ü§î. By selecting these distinct emojis, I hoped to give the classifier a better chance at accurately predicting the sentiment behind each tweet.</p>
<h2 id="preprocessing">Preprocessing</h2>
<p>In the dataset, each tweet is separated with one word per line, and each word ends with an &ldquo;O&rdquo; unless there&rsquo;s an emoji, in which case the line ends with the emoji üìù. Each tweet is also delimited by START and STOP tokens that have an &ldquo;O&rdquo; attached. To make the data more manageable, I first reconstructed the tweets from this format into complete sentences. During this process, I noticed that some tweets contained more than one emoji, so I only kept the content up until the first emoji that appeared üöß. If a tweet started with an emoji, it resulted in an empty sentence, which I discarded. Next, I performed some basic preprocessing, such as lowercasing and removing symbols, and filtered the tweets to keep only those containing my chosen emojis.</p>
<p>As I mentioned earlier, Twitter users tend to have quite creative spelling üé®. I wanted to tackle this issue by using machine learning to correct spelling errors. I found a project called <a href="https://github.com/neuspell/neuspell">Neuspell</a> that aims to do just that. Unfortunately, I encountered difficulties running the project and, when I finally got it to work, it was too slow for my needs (taking about a second per tweet, and I have hundreds of thousands) ‚è≥.</p>
<h2 id="model">Model</h2>
<p>Initially, this post was going to compare Transformers and RNNs, but I struggled to train a Transformer successfully ü§ñ. Despite trying finetuning, no finetuning, and battling OOM errors in both RAM and GPU, I only achieved a slow-improving model that seemed like it would never finish. So, I abandoned the Transformer and decided to explore an architecture with GRUs and Attention layers instead.</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>  inputs = Input(shape=(<span style="color:#0086f7;font-weight:bold">32</span>))
</span></span><span style="display:flex;"><span>  embedding = Embedding(input_dim=<span style="color:#0086f7;font-weight:bold">4096</span>, output_dim=<span style="color:#0086f7;font-weight:bold">1024</span>, input_length=max_length)(inputs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  out_layer = embedding
</span></span><span style="display:flex;"><span>  <span style="color:#fb660a;font-weight:bold">for</span> i in range(<span style="color:#0086f7;font-weight:bold">2</span>):
</span></span><span style="display:flex;"><span>    gru = GRU(<span style="color:#0086f7;font-weight:bold">1024</span>, activation=<span style="color:#0086d2">&#39;relu&#39;</span>, return_sequences=<span style="color:#fb660a;font-weight:bold">True</span>)(out_layer)
</span></span><span style="display:flex;"><span>    batch_norm = BatchNormalization()(gru)
</span></span><span style="display:flex;"><span>    dropout = Dropout(<span style="color:#0086f7;font-weight:bold">0.5</span>)(batch_norm)
</span></span><span style="display:flex;"><span>    attention = Attention()([dropout, dropout])
</span></span><span style="display:flex;"><span>    out_layer = Concatenate(axis=-<span style="color:#0086f7;font-weight:bold">1</span>)([dropout, attention])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  gru = GRU(<span style="color:#0086f7;font-weight:bold">512</span>, activation=<span style="color:#0086d2">&#39;relu&#39;</span>)(out_layer)
</span></span><span style="display:flex;"><span>  batch_norm = BatchNormalization()(gru)
</span></span><span style="display:flex;"><span>  dropout = Dropout(<span style="color:#0086f7;font-weight:bold">0.5</span>)(batch_norm)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  outputs = Dense(<span style="color:#0086f7;font-weight:bold">5</span>, activation=<span style="color:#0086d2">&#39;softmax&#39;</span>)(dropout)</span></span></code></pre></div>
<p>This architecture isn&rsquo;t overly complex. The layer sizes and overall structure are quite experimental üß™. Although I read that using a &rsquo;tanh&rsquo; activation for GRU layers was recommended, I found that &lsquo;relu&rsquo; worked better in my case. Adding Attention layers proved to be a significant improvement üìà.</p>
<p>Here is the summary:</p>
<pre tabindex="0"><code>__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 32)]         0           []                               
                                                                                                  
 embedding (Embedding)          (None, 32, 1024)     4194304     [&#39;input_1[0][0]&#39;]                
                                                                                                  
 gru (GRU)                      (None, 32, 1024)     6297600     [&#39;embedding[0][0]&#39;]              
                                                                                                  
 batch_normalization (BatchNorm  (None, 32, 1024)    4096        [&#39;gru[0][0]&#39;]                    
 alization)                                                                                       
                                                                                                  
 dropout (Dropout)              (None, 32, 1024)     0           [&#39;batch_normalization[0][0]&#39;]    
                                                                                                  
 attention (Attention)          (None, 32, 1024)     0           [&#39;dropout[0][0]&#39;,                
                                                                  &#39;dropout[0][0]&#39;]                
                                                                                                  
 concatenate (Concatenate)      (None, 32, 2048)     0           [&#39;dropout[0][0]&#39;,                
                                                                  &#39;attention[0][0]&#39;]              
                                                                                                  
 gru_1 (GRU)                    (None, 32, 1024)     9443328     [&#39;concatenate[0][0]&#39;]            
                                                                                                  
 batch_normalization_1 (BatchNo  (None, 32, 1024)    4096        [&#39;gru_1[0][0]&#39;]                  
 rmalization)                                                                                     
                                                                                                  
 dropout_1 (Dropout)            (None, 32, 1024)     0           [&#39;batch_normalization_1[0][0]&#39;]  
                                                                                                  
 attention_1 (Attention)        (None, 32, 1024)     0           [&#39;dropout_1[0][0]&#39;,              
                                                                  &#39;dropout_1[0][0]&#39;]              
                                                                                                  
 concatenate_1 (Concatenate)    (None, 32, 2048)     0           [&#39;dropout_1[0][0]&#39;,              
                                                                  &#39;attention_1[0][0]&#39;]            
                                                                                                  
 gru_2 (GRU)                    (None, 512)          3935232     [&#39;concatenate_1[0][0]&#39;]          
                                                                                                  
 batch_normalization_2 (BatchNo  (None, 512)         2048        [&#39;gru_2[0][0]&#39;]                  
 rmalization)                                                                                     
                                                                                                  
 dropout_2 (Dropout)            (None, 512)          0           [&#39;batch_normalization_2[0][0]&#39;]  
                                                                                                  
 dense (Dense)                  (None, 5)            2565        [&#39;dropout_2[0][0]&#39;]              
                                                                                                  
==================================================================================================
Total params: 23,883,269
Trainable params: 23,878,149
Non-trainable params: 5,120
</code></pre><h2 id="running-environments">Running Environments</h2>
<p>I&rsquo;d like to mention that for this project, I experimented with three different environments: my local PC (good CPU, 48GB RAM, 6GB GPU), Google Colab, and Kaggle Notebooks üñ•Ô∏è.</p>
<p>Each option has its pros and cons. While my PC isn&rsquo;t as powerful as the other two instances, I can run my model overnight without worrying about being disconnected. Google Colab and Kaggle offer less RAM (around 15GB) but more powerful GPUs. However, I can&rsquo;t run a process for too long on them. The real game-changer comes in the form of TPUs.</p>
<p>So, what are TPUs? TPUs (Tensor Processing Units) are specialized hardware accelerators designed by Google specifically for machine learning üí°. They are optimized to perform matrix operations and are especially effective for training and running large neural networks. TPUs can provide significant speedups compared to traditional CPUs and GPUs, making them an invaluable resource for accelerating machine learning applications.</p>
<p>While I can&rsquo;t afford to buy a TPU since I&rsquo;m not a millionaire yet üí∏, both Colab and Kaggle allow you to use theirs. So what&rsquo;s the difference? My model on a GPU takes one hour per epoch, but on a TPU&hellip; it takes 70 seconds ‚ö°.</p>
<h2 id="setting-up-tpus">Setting up TPUs</h2>
<p>First of all, how do you gain access to a TPU? ü§î</p>
<p>Kaggle makes it easy by providing it as an option right from the start. Simply change the Accelerator to TPU VM v3-8 in the Notebook options. There&rsquo;s a high demand for these instances, so you might have to wait a bit ‚Äì on average, I waited 15 minutes. If you&rsquo;re #20 or less in the queue, just be patient; it&rsquo;ll be worth your while. Another great feature is that the instance comes with 330GB RAM (yes, you read that correctly) üéâ.</p>
<p>For Google Colab, you&rsquo;ll need to request access <a href="https://sites.research.google/trc/about/">here</a>. I got it on the same day. The advantage here is that there&rsquo;s less demand, and it&rsquo;s unlikely that you&rsquo;ll have to wait for the TPU. However, you won&rsquo;t have 330GB of RAM, just the usual 15GB-ish.</p>
<p>Once you have an environment with a TPU, setting up the code is a breeze. Just execute this once:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()
</span></span><span style="display:flex;"><span>tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)</span></span></code></pre></div>
<p>And then wrap your model compilation within this statement:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#fb660a;font-weight:bold">with</span> tpu_strategy.scope():
</span></span><span style="display:flex;"><span>    model = Sequential()
</span></span><span style="display:flex;"><span>    ...
</span></span><span style="display:flex;"><span>    ...
</span></span><span style="display:flex;"><span>    model.compile(...)</span></span></code></pre></div>
<p>And that&rsquo;s it üöÄ.</p>
<p>Now, with a TPU comes great power: you can use a large batch_size like 1024 and parallel batches with the steps_per_execution parameter in the compile method:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model.compile(optimizer=optimizer, 
</span></span><span style="display:flex;"><span>			  loss=loss_fn, 
</span></span><span style="display:flex;"><span>			  metrics=metrics, 
</span></span><span style="display:flex;"><span>			  steps_per_execution=<span style="color:#0086f7;font-weight:bold">32</span>)</span></span></code></pre></div>
<p>And we can draw more power. Kaggle documentation on TPUs say:</p>
<pre tabindex="0"><code>Because TPUs are very fast, many models ported to TPU end up with a data bottleneck. 
The TPU is sitting idle, waiting for data for the most part of each training epoch. 
</code></pre><p>Even with all our previous efforts, the TPU can still be idle (!). Kaggle TPUs read from GCS (Google Cloud Storage) and their solution to this bottleneck is to feed the TPU with several GCS files in parallel. I haven&rsquo;t tried this approach, so I&rsquo;m not sure if it&rsquo;s easy or not. However, it&rsquo;s certainly a potential method to explore if you want to further optimize your TPU utilization.</p>
<h2 id="results">Results</h2>
<p>I trained the model for 30 epochs, and thanks to the TPU, each epoch took only 70 seconds üöÄ. The validation accuracy I achieved was 0.6 without Attention layers and 0.7 with Attention. Another thing I tried was measuring the accuracy of the top 2 results, since two emojis could be acceptable for a given text ü§î. This approach yielded an accuracy of around 0.85, but I ultimately decided to stick with the traditional accuracy measure.</p>
<p>Here are some results predicted by the model from the validation set:</p>
<p><img src="/emoji2.png" alt="Results"></p>
<p>And here is a csv extract of the results, if you want to explore</p>
<p><a href="https://github.com/nicovaras/deep-learning-projects/blob/main/emoji/results_emoji.csv">CSV Extract</a></p>
<h2 id="conclusion">Conclusion</h2>
<p>In conclusion, TPUs have been a game changer for this project and I can confidently say that I&rsquo;ll be using them in future projects as well üåü. The incredible speedup they provide allows me to train models much faster, making experimentation and iteration far more feasible.</p>
<p>The accuracy of the emoji classification model, while not perfect, is still acceptable given the nature of the task and the inherent challenges of dealing with social media text. However, it is important to remember that the primary goal of this post was to showcase the power of TPUs, rather than solely focusing on the emoji classification problem.</p>
<p>Thanks for reading!</p>
<p>Notebook: <a href="https://github.com/nicovaras/deep-learning-projects/blob/main/emoji.ipynb">here</a></p>

            </div>
        </article>
    </main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
<a href="https://github.com/nicovaras" target="_blank" rel="noopener noreferrer me" title="Github">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
</a></div>
    <small class="footer_copyright">
        ¬© 2023 Nico Varas.
        Powered by <a href="https://github.com/hugo-sid/hugo-blog-awesome" target="_blank" rel="noreferrer noopener">Hugo blog awesome</a>
        theme on
        <a href="https://gohugo.io" target="_blank" rel="noreferrer noopener">Hugo</a>.
    </small>
</footer>
<script src="https://nicovaras.github.io/js/themeSwitchnMenu.min.2a402288242b6930b175a0722c267e2353055739b3975834df35e56d00dd8f50.js"></script></body>
</html>
